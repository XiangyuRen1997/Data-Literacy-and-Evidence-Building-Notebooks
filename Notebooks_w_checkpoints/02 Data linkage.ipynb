{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1jYI-hwH7YnD"
   },
   "source": [
    "# Data Linkage\n",
    "\n",
    "**Tian Lou** \\\n",
    "Ohio Education Research Center \\\n",
    "The Ohio State University\n",
    "\n",
    "**Xiangyu Ren** \\\n",
    "New York University\n",
    "\n",
    "**Anna-Carolina Haensch** \\\n",
    "University of Maryland \\\n",
    "LMU Munich\n",
    "\n",
    "[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.10263021.svg)](https://doi.org/10.5281/zenodo.10263021)\n",
    "\n",
    "**This notebook is developed for the [Data Literacy and Evidence Building Executive Class](https://www.socialdatascience.umd.edu/data-literacy).**\n",
    "\n",
    "**The \"Syntucky\" data, which is synthetic in nature, is exclusively designed for training exercises. It is not intended to derive meaningful insights or make determinations about real-world populations.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LMJqROtO7YnH"
   },
   "source": [
    "## Goals\n",
    "\n",
    "Data linkage is a key part of data science. It involves combining multiple data sources to get a more comprehensive understanding of an individual's experience. Recall that in the Data Exploration notebook, we explored the 2015 Syntucky cohort data, which includes demographic information, college enrollment and graduation, and labor market outcomes of degree pursuers who entered college for the first time in 2015. While in this class we have convenient access to all these pieces of information in one dataset, in reality, we need to do a tremedous amount of work to pre-process, clean, format, and link various education and employment datasets. \n",
    "\n",
    "In this notebook, we introduce you to a few data linkage methods and show you how different methods can generate different samples. Specifically, we will link three datasets about *Syntucky students who enrolled in college in 2015*: 1) a **master dataset**, which contains demographic information; 2) an **employment dataset**, which reveals labor market information during the seventh year after college entrance; 3) an **education dataset**, which provides details about a student's higher education background.\n",
    "\n",
    "**The specific questions we seek to answer in this notebook are**:\n",
    "1. What are the differences between left, right, and inner joins? \n",
    "2. How do different join methods affect the sample size and missingness of the final data?\n",
    "3. When do we need to use left/right/inner join?\n",
    "\n",
    "**After completing this notebook, you should:**\n",
    "1. Learn how to join cross-sectional datasets in order to create a dataset that contains the key information about an individual.\n",
    "2. Become familar with SQL commands, `LEFT JOIN` and `INNER JOIN`, and Python Pandas function, `merge()`.\n",
    "3. Understand how different join methods would affect your final data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_mZHRtb7YnI"
   },
   "source": [
    "## 1. Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6RC8zKC87YnJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "# Interface to connect mySQL database server in Python\n",
    "import MySQLdb \n",
    "\n",
    "# Library that provides lightweight disk-based database\n",
    "import sqlite3 \n",
    "\n",
    "# File system path\n",
    "from pathlib import Path\n",
    "\n",
    "# Data manipulation and analysis tool\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzyV4w7B7YnK"
   },
   "source": [
    "As mentioned above, in order to create a dataset that includes a person's demographics, education, and employment histories, we will link three datasets:  \n",
    "\n",
    "**Master Dataset**: This dataset contains **demographic information about the synthetic individuals that enrolled in college in 2015**. (They may enter college during any calendar year between 2005 and 2015). The variables in this dataset are time-invariant, meaning they do not change over time. It includes birth year, birth month, gender, race, underrepresented minority (URM) status, and in-state origin indicator. \n",
    "\n",
    "**Employment Dataset**: This dataset provides information about **Syntucky individuals' labor market status** during the seventh year after college entrance. **This dataset only includes people who were employed during the seventh year after college entrance**. \n",
    "\n",
    "**Education Dataset**: This dataset holds information regarding **college entrance, enrollment, and degrees obtained**. It includes details such as the year a person first entered college as degree pursuers, the majors they enrolled in, the highest degree they obtained, etc.\n",
    "\n",
    "Through the process of joining these datasets, a more comprehensive and cohesive depiction of the synthetic units' life journey can be obtained. The simple **ER diagram** below shows the relationships between the three datasets. We can see that the common column that we can use to join any two of these datasets should be `id`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![diagram.png](diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the code below, please change <font color='red'> **YOUR DATA DIRECTORY**</font> to your own file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E1yOgKoK7YnM"
   },
   "outputs": [],
   "source": [
    "#Define data directory\n",
    "data_directory = 'YOUR DATA DIRECTORY'\n",
    "\n",
    "#master data\n",
    "master_df = pd.read_csv(data_directory + 'master_crosssection.csv')\n",
    "\n",
    "#employment data\n",
    "employment_df = pd.read_csv(data_directory + 'employment_crosssection.csv')\n",
    "\n",
    "#education data\n",
    "education_df = pd.read_csv(data_directory + 'education_crosssection.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6lc4iFLs7YnM"
   },
   "source": [
    "Now, let's take a close look at the number of rows in each dataset and what columns they contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BnTbmOTf7YnN",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Master data\n",
    "print('The master data has', master_df.shape[0], 'rows.')\n",
    "\n",
    "master_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Employment data\n",
    "print('The employment data has', employment_df.shape[0], 'rows.')\n",
    "\n",
    "employment_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Education data\n",
    "print('The education data has', education_df.shape[0], 'rows.')\n",
    "\n",
    "education_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HEgokMaf7YnN"
   },
   "source": [
    "## 2. Establish Database Connection and Load Data to the Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESMxStLu7YnO"
   },
   "source": [
    "Utilizing SQL in Python opens up new dimensions for data handling and manipulation, providing powerful querying capabilities. Here's the process we use to transfer data from a CSV file to a database:\n",
    "\n",
    "1. **Create a Database**: We use Python code `Path ('C:/Users/*YOUR FOLDERNAME*/syn_data.db').touch()` to create a new SQLite database file named `syn_data.db`. It won't modify the file if it does exist, making this a safe operation to perform even if you're unsure about the file's existence. \n",
    "\n",
    "> Before running the code below, please change <font color='red'> **YOUR USERNAME**</font> to your username or use your own file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4wnjNLXx7YnO",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Change your working path to your personal folder\n",
    "Path('C:/Users/YOUR USERNAME/syn_data.db').touch() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXOUpAYK7YnP"
   },
   "source": [
    "2. **Establish the Database Connection**: Then we need to tell Python to connect to our database, which is like opening a book to start reading it. Once our \"book\" (database) is open, we need a way to navigate through it. In Python, we do this by using `cursor()`. You can think of the cursor like a finger pointing at a line in the book, ready to read or change the words. The code `c = conn.cursor()` creates the cursor and the abbreviation of the cursor is `c`.\n",
    "\n",
    " > Before running the code below, please change <font color='red'> **YOUR USERNAME**</font> to your username or use your own file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ran0-GNI7YnP"
   },
   "outputs": [],
   "source": [
    "# Establish database connection\n",
    "conn = sqlite3.connect('C:/Users/YOUR USERNAME/syn_data.db') \n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESMxStLu7YnO"
   },
   "source": [
    "3. **Create a Table**: The next step is to create a table within the database to house the data from our CSV files. We use `CREATE TABLE tblname` to create tables, where `tblname` is the name of your table. This process requires a list of column definitions, including column name and type.\n",
    "\n",
    "   The code below, `c.execute('''CREATE TABLE master(...) ''')`, runs a SQL query that creates a new table named *master* in our database. In the *master* table, we are specifying six columns: *id*, *gender*, *birth_year*, *birth_month*, *urm_status*, *race_group*, and *instate_origin*. We assign each of these columns a specific data type: *id*, *birth_year*, and *instate_origin* are integers, represented by `int`, while *gender*, *birth_month*, *urm_status*, and *race_group* will contain text, represented by `text`. The command is wrapped in triple quotes, which is a Python convention for defining multi-line strings, but in this case, it's used for a single line of string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e93xcKJr7YnQ"
   },
   "outputs": [],
   "source": [
    "#Remove the table if already exist\n",
    "c.execute('''DROP TABLE IF EXISTS master ''')\n",
    "\n",
    "#Create an empty table, \"master\", in your database\n",
    "#In the code below, we define the column names and types before we can upload the master data to the database\n",
    "c.execute('''CREATE TABLE master (id int, \n",
    "                                  gender text, \n",
    "                                  birth_year int, \n",
    "                                  birth_month text,\n",
    "                                  urm_status text,\n",
    "                                  race_group text,\n",
    "                                  instate_origin  int)''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Load Data**: The next step is to populate the table with data. The code `master_df.to_sql('master',conn, if_exists = 'replace',index = False)` is a method from Pandas. It is used to write records stored in a DataFrame to a SQL table. It takes several arguments, including the name of the SQL table, the connection instance to the database, and some optional parameters. In this case, `if_exists = 'replace'` indicates that if the table already exists in the database, the new data should replace the existing data. The `index = False` argument tells the function not to write row indices from the DataFrame into the SQL table. So, this command is essentially saying, \"Take the data in our `master_df` DataFrame and replace the old table with the *master* table in our database, but don't worry about including the DataFrame's indices.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i7xuleWJ7YnQ"
   },
   "outputs": [],
   "source": [
    "#Load data in master_df to the database\n",
    "master_df.to_sql('master', conn, if_exists = 'replace', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By following  the above steps, we'll be able to effectively move our data into a SQL database and execute SQL commands within our Python environment. Now let's repeat the above steps for the employment and education datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_OFWVBJr7YnR"
   },
   "outputs": [],
   "source": [
    "#Remove the table if already exist\n",
    "c.execute('''DROP TABLE IF EXISTS employment ''')\n",
    "\n",
    "#Create the employment table in the database\n",
    "c.execute('''CREATE TABLE employment (id int,\n",
    "                                      year7_max_qtrs_one_employer int, \n",
    "                                      year7_education_industry_employed int, \n",
    "                                      year7_ct_qtrs_employed int,\n",
    "                                      year7_ct_employers int, \n",
    "                                      year7_earnings int, \n",
    "                                      year7_earnings_most_consistent_employer int)''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_OFWVBJr7YnR"
   },
   "outputs": [],
   "source": [
    "#Load employment data to the database\n",
    "employment_df.to_sql('employment', conn, if_exists = 'replace', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ipnDpWtu7YnR"
   },
   "outputs": [],
   "source": [
    "#Remove the table if already exist\n",
    "c.execute('''DROP TABLE IF EXISTS education ''')\n",
    "\n",
    "#Create the education table in the database\n",
    "c.execute('''CREATE TABLE education (id int,\n",
    "                                     first_enroll text,\n",
    "                                     first_enroll_term text,\n",
    "                                     first_enroll_calendaryear int,\n",
    "                                     high_completion_acadyr int,\n",
    "                                     high_completion_label text,\n",
    "                                     high_completion text,\n",
    "                                     year7_enrolled int)''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the education data to the database\n",
    "education_df.to_sql('education',conn, if_exists = 'replace',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENW2HlDt7YnS"
   },
   "source": [
    "## 3. Join Datasets in SQL\n",
    "\n",
    "In SQL, you have several options to join datasets. First, `LEFT JOIN` takes all the rows from the left dataset, and combines them with the matching rows from the right dataset. If there is no match in the right dataset, the result is `NaN` in the columns of the right dataset. A left join will ensure that all records from the left dataset are retained, even if there's no corresponding record in the right dataset. \n",
    "\n",
    "The example below illustrates how **left join** works by using *simplified versions* of the master data and the employment data. We can see that since person 3 is not in the employment data, after we left join the master data with the employment data, person 3 remains in the data but his year 7 earnings is `NaN`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<text><center>**Master Data**</center></text>\n",
    "\n",
    "|id|gender|birth_year|\n",
    "|:--------:|:--------:|:--------|\n",
    "|1|female|1996|\n",
    "|2|male|1993|\n",
    "|3|male|2000|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<text><center>**Employment Data**</center></text>\n",
    "\n",
    "|id|year7_earnings|\n",
    "|:--------:|:--------:|\n",
    "|1|50000|\n",
    "|2|60000|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<text><center>**Left Join the Master Data and the Employment Data**</center></text>\n",
    "\n",
    "|id|gender|birth_year|year7_earnings|\n",
    "|:--------:|:--------:|:--------:|:--------:|\n",
    "|1|female|1996|50000|\n",
    "|2|male|1993|60000|\n",
    "|3|male|2000|NaN|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_zSaRxX7YnS"
   },
   "source": [
    "Let's do it in SQL. The `SELECT` statement is a fundamental SQL command used to fetch data from a database. It allows you to choose specific data from one or more tables by defining the columns you want to include. When you use `SELECT *`, the asterisk (*) is a wildcard character that represents 'all'. In the context of an SQL query, it means \"select all columns\". \n",
    "\n",
    "Can you spot the `LEFT JOIN` and `ON` in the next two lines? The SQL query is saying, \"Select all columns from the *master* table, and then join it with the *employment* table, using `id` as the common column\". \n",
    "\n",
    "The `pd.read_sql()` function is used to perform an SQL query, and to store the result in a new DataFrame named `master_employment_left_df`. The function takes two main arguments: the SQL query as a string, and the connection instance to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KRDcPUoi7YnS"
   },
   "outputs": [],
   "source": [
    "#Left join master data and employment data by using SQL query\n",
    "master_employment_left_df = pd.read_sql('''SELECT * FROM master m \n",
    "                                           LEFT JOIN employment e \n",
    "                                           ON m.id = e.id''', conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the number of rows of `master_employment_left_df` and its columns. We can see that it has the same number of rows as `master_df`. However, many people have missing year 7 earnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XiD2qsOD7YnT"
   },
   "outputs": [],
   "source": [
    "#Check number of rows\n",
    "print('master_df has', master_df.shape[0], 'rows.')\n",
    "print('master_employment_left_df has', master_employment_left_df.shape[0], 'rows.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check year 7 earnings missingness\n",
    "master_employment_left_df['year7_earnings'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8JlMW_B7YnT"
   },
   "source": [
    "Next, let's look at how **inner join** works. Inner join will only keep values that are matched in both datasets and drop all the rows that are not matched. The corresponding SQL command is `INNER JOIN`. Using the simplified example from earlier, the inner joined table would not have person 3's records.\n",
    "\n",
    "<text><center>**Inner Join the Master Data and the Employment Data**</center></text>\n",
    "\n",
    "|id|gender|birth_year|year7_earnings|\n",
    "|:--------:|:--------:|:--------:|:--------:|\n",
    "|1|female|1996|50000|\n",
    "|2|male|1993|60000|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SWMattHl7YnU"
   },
   "outputs": [],
   "source": [
    "#Inner join master data and employment data by using SQL query\n",
    "master_employment_inner_df = pd.read_sql('''SELECT * FROM master m \n",
    "                                            INNER JOIN employment e \n",
    "                                            ON m.id = e.id''', conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that by using `INNER JOIN`, the resulting DataFrame `master_employment_inner_df` has fewer rows than `master_df`. However, we do not have missing year 7 earnings in the `master_employment_inner_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Prhl_azd7YnU"
   },
   "outputs": [],
   "source": [
    "#Check number of rows\n",
    "print('master_df has', master_df.shape[0], 'rows.')\n",
    "print('employment_df has', employment_df.shape[0], 'rows.')\n",
    "print('master_employment_inner_df has', master_employment_inner_df.shape[0], 'rows.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check year 7 earnings missingness\n",
    "master_employment_inner_df['year7_earnings'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When should you use left join and when should you use inner join?** It depends. If you want to study the whole population in the master data, you should always left join the master data with other datasets to ensure that you keep all the people in the master data. However, if you are only interested in people who have earnings in year 7, you can perform inner join or right join the master data with the employment data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Checkpoint 1: Join the Master Data and the Education Data**\n",
    "\n",
    "Please join the master table with the education table by using SQL command `LEFT JOIN` and `INNER JOIN`. Check how the number of rows changes in the final DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vanki9ZS7Yne"
   },
   "source": [
    "## 4. Data Linkage using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oeUrIVeN7Ynj"
   },
   "source": [
    "The `merge` function from the pandas library is a powerful tool in Python for joining or combining data from different datasets based on common columns. It replicates various types of SQL joins. SQL is often preferred for combining larger datasets or performing complex queries, mainly because of its efficiency. However, Python joins remain useful. Once data has been filtered or aggregated to a manageable size in SQL, Python's pandas library provides a more flexible and intuitive interface for further data manipulation and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, we use the `merge()` function to left join and inner join the master data and the employment data. In the `merge()` function, you need to define the DataFrame you want to join the `master_df` with, the join method in `how = `, and the common columns used for joining in `on = `."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8vk3gvCt7Ynk"
   },
   "outputs": [],
   "source": [
    "#Left Join in Python\n",
    "master_emp_pd_left_df = master_df.merge(employment_df, how = 'left', on = 'id')\n",
    "\n",
    "#Inner Join in Python\n",
    "master_emp_pd_inner_df = master_df.merge(employment_df, how = 'inner', on = 'id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8zHvjcKT7Ynk"
   },
   "source": [
    "Let us now compare the joining results by using `merge()` in Python with the results by using SQL commands. We can see that we get the exact same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KuK0Eglo7Ynl"
   },
   "outputs": [],
   "source": [
    "#Compare results\n",
    "print('In SQL, after left joining master data with employment data, our final number of rows is', \n",
    "      master_employment_left_df.shape[0], '.')\n",
    "print('In Python, after left joining master data with employment data, our final number of rows is', \n",
    "      master_emp_pd_left_df.shape[0], '.')\n",
    "\n",
    "print('In SQL, after inner joining master data with employment data, our final number of rows is', \n",
    "      master_employment_inner_df.shape[0], '.')\n",
    "print('In Python, after inner joining master data with employment data, our final number of rows is', \n",
    "      master_emp_pd_inner_df.shape[0], '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QOKj7pyK7Ynl"
   },
   "source": [
    "#### **Checkpoint 2: Right Join the Master Data and the Employment Data**\n",
    "\n",
    "Please right join the master data, `master_df`, with the employment data, `employment_df`, by using pandas function `merge()`. Check how the number of rows changes in the final DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

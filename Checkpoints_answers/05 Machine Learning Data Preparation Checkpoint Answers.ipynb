{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning: Data Preparation Checkpoint Answers\n",
    "\n",
    "**Tian Lou** \\\n",
    "Ohio Education Research Center \\\n",
    "The Ohio State University\n",
    "\n",
    "**Xiangyu Ren** \\\n",
    "New York University\n",
    "\n",
    "[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.10257277.svg)](https://doi.org/10.5281/zenodo.10257277)\n",
    "\n",
    "**This notebook is developed for the [Data Literacy and Evidence Building Executive Class](https://www.socialdatascience.umd.edu/data-literacy).**\n",
    "\n",
    "**The \"Syntucky\" data, which is synthetic in nature, is exclusively designed for training exercises. It is not intended to derive meaningful insights or make determinations about real-world populations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Analysis Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Machine Learning Packages\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the code below, please change <font color='red'> **YOUR DATA DIRECTORY**</font> to your own file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load 2013, 2014, and 2015 data\n",
    "\n",
    "#Define data folder directory\n",
    "data_directory = 'YOUR DATA DIRECTORY'\n",
    "\n",
    "#Read in different cohort data\n",
    "df_2013 = pd.read_csv(data_directory+'syntucky_cohort_2013.csv')\n",
    "df_2014 = pd.read_csv(data_directory+'syntucky_cohort_2014.csv')\n",
    "df_2015 = pd.read_csv(data_directory+'syntucky_cohort_2015.csv')\n",
    "\n",
    "#Combine them into one dataset for easier cleaning and generating variables\n",
    "df_comb = pd.concat([df_2013,df_2014,df_2015])\n",
    "\n",
    "#Check the first five rows of the combined data\n",
    "df_comb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Checkpoint 1: Use an Alternative Job Quality Measurement to Create the Label**\n",
    "\n",
    "In the data measurement notebook, we developed several job quality measures, including number of jobs (`year7_ct_employers`), employment duration (`year7_ct_qtrs_employed`), and average earnings per employed quater (`year7_earnings` / `year7_ct_qtrs_employed`). Please use one of these measurements or your own measurement to create the job quality label. Also check the distribution of your label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Consistent job Label\n",
    "# =1 if have 1 employer\n",
    "# =0 if have more than 1 employers\n",
    "#Exclude:\n",
    "#Students who are not employed in year 7\n",
    "conditions = [((df_comb['year7_ct_employers'].notna()) & (df_comb['year7_ct_employers'] > 1) & (df_comb['first_enroll_acadyr_pell_disbursed'].isnull() == False)),\n",
    "              ((df_comb['year7_ct_employers'] == 1) & (df_comb['first_enroll_acadyr_pell_disbursed'].isnull() == False))]\n",
    "\n",
    "choices = [0,\n",
    "           1]\n",
    "\n",
    "df_comb['label_consistent_jobs'] = np.select(conditions, choices, default = np.NaN)\n",
    "\n",
    "#Check label distribution\n",
    "df_consistent_jobs = df_comb.groupby(['label_consistent_jobs'])['id'].agg(['count']).reset_index()\n",
    "\n",
    "df_consistent_jobs['percent'] = round(df_consistent_jobs['count'] / df_consistent_jobs['count'].sum(),2)\n",
    "\n",
    "df_consistent_jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Checkpoint 2: Create additional Features**\n",
    "\n",
    "Try to create additional features. For example, you can add `first_enroll_fulltime` to the feature list or use `urm_status` as a feature instead of `race_group`. Think about if your new features are categorical or numeric and how you should process them. Also, check if your new features have missing values and think about how you should deal with these missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Cleaning\n",
    "\n",
    "#Remove doctoral degree recipients due to potential data error\n",
    "df_comb = df_comb[df_comb['high_completion_label'] != 'Doctoral']\n",
    "\n",
    "#Check the number of unknown gender\n",
    "print(df_comb.groupby(['gender'])['id'].agg(['count']).reset_index())\n",
    "\n",
    "#Remove unknown gender due to small number\n",
    "df_comb = df_comb[df_comb['gender'] != \"Unknown\"]\n",
    "\n",
    "#Generate new features\n",
    "\n",
    "#Age at year 7\n",
    "df_comb['year7_age'] = df_comb['cohort_acadyr'] + 6 - df_comb['birth_year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only keep the columns we need\n",
    "\n",
    "#Identifiers\n",
    "id_cols = ['id', 'cohort_acadyr']\n",
    "\n",
    "#Labels\n",
    "label_cols = ['label_consistent_jobs']\n",
    "\n",
    "#Categorical features\n",
    "cat_cols = ['first_enroll_acadyr_pell_disbursed', 'gender', 'urm_status', 'instate_origin', 'first_enroll', \n",
    "            'cohort_degree_pursuit_type']\n",
    "\n",
    "#Numeric features\n",
    "num_cols = ['year7_age']\n",
    "\n",
    "#Only keep the features we need\n",
    "df_comb = df_comb[id_cols + label_cols + cat_cols + num_cols]\n",
    "\n",
    "#Check the current data\n",
    "df_comb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check number of null values in labels and features\n",
    "#Note that this method only check if a column has null value. However, missing values could be in many formats.\n",
    "#You may want to inspect further depending on the data you use.\n",
    "for col in label_cols + cat_cols + num_cols:\n",
    "    print(col, df_comb[col].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill in categorical features' null values with \"Missing\"\n",
    "df_comb.loc[df_comb['first_enroll'].isnull() == True, 'first_enroll'] = \"Missing\"\n",
    "\n",
    "#Fill in missing age with the average age at year 7. Also bottom and top code age\n",
    "df_comb.loc[df_comb['year7_age'].isnull() == True, 'year7_age'] = round(df_comb['year7_age'].mean(), 0)\n",
    "\n",
    "df_comb.loc[df_comb['year7_age'] < 16, 'year7_age'] = 16\n",
    "\n",
    "df_comb.loc[df_comb['year7_age'] > 64, 'year7_age'] = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Categorical features that we need to convert to dummy variables\n",
    "cat_cols = ['gender', 'urm_status', 'first_enroll', 'cohort_degree_pursuit_type']\n",
    "\n",
    "#Get dummy variables\n",
    "df_comb = pd.get_dummies(df_comb, columns = cat_cols, dtype = float)\n",
    "\n",
    "#Check our current columns\n",
    "df_comb.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define scaler type\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "#Compute the minimum and maximum to be used for scaling\n",
    "scaler.fit(df_comb['year7_age'].values.reshape(-1,1))\n",
    "\n",
    "#Scaling features to range [0, 1]\n",
    "df_comb['year7_age_scl'] = scaler.transform(df_comb['year7_age'].values.reshape(-1,1))\n",
    "\n",
    "#Drop the original numeric feature\n",
    "df_comb.drop(columns = ['year7_age'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Check summary descriptive statistics for all labels and features\n",
    "#We use `.T` to tranpose the table so that it is easier to read\n",
    "#The code in `.apply()` is to format the numbers in the table so that they only have five digits after the decimal\n",
    "df_comb.describe().T.apply(lambda x: x.apply('{0:.5f}'.format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
